#!/usr/bin/env python3
"""
JSON to Neo4j Converter

This script converts LlamaParse JSON output to Neo4j Cypher commands for importing
contract data into a graph database. Uses SpaCy and ContractBERT for enhanced legal text processing.
"""

import argparse
import json
import os
import sys
import re
import spacy
from typing import Dict, List, Any, Optional
from datetime import datetime

# Import ContractBERT/transformer libraries
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline, AutoModelForSequenceClassification

# Import constants from contract_constants.py
from contract_constants import (
    DEFINITION_PATTERNS, OBLIGATION_PATTERNS, CONDITION_PATTERNS,
    TERM_PATTERNS, PAYMENT_PATTERNS
)

# Import extraction functions from extract package
from extract import extract_contract_metadata, extract_articles, extract_parties

# Import Cypher generation functionality
from cypher_generator import process_json_file

# Load SpaCy model
nlp = spacy.load("en_core_web_sm")

# Global matchers for identifying contract elements
matcher = spacy.matcher.Matcher(nlp.vocab)
phrase_matcher = spacy.matcher.PhraseMatcher(nlp.vocab)

# Global variables for ContractBERT models
contractbert_ner = None
contractbert_classifier = None
contractbert_tokenizer = None

def initialize_contractbert():
    """Initialize ContractBERT models for legal text analysis."""
    global contractbert_ner, contractbert_classifier, contractbert_tokenizer
    
    try:
        print("Loading ContractBERT models...")
        # Initialize tokenizer using legal-bert as base
        contractbert_tokenizer = AutoTokenizer.from_pretrained("nlpaueb/legal-bert-base-uncased")
        
        # Load NER model for entity extraction
        ner_model = AutoModelForTokenClassification.from_pretrained("nlpaueb/legal-bert-base-uncased")
        contractbert_ner = pipeline("ner", model=ner_model, tokenizer=contractbert_tokenizer, 
                                    aggregation_strategy="simple")
        
        # Load classifier for document and clause classification
        classifier_model = AutoModelForSequenceClassification.from_pretrained("nlpaueb/legal-bert-base-uncased")
        contractbert_classifier = pipeline("text-classification", model=classifier_model, 
                                         tokenizer=contractbert_tokenizer)
        
        print("ContractBERT models loaded successfully")
        return True
    except Exception as e:
        print(f"Warning: Could not initialize ContractBERT models: {e}")
        print("Falling back to SpaCy-only analysis")
        return False

# Initialize patterns for contract element detection
def initialize_matchers():
    """Initialize the pattern matchers for contract elements."""
    # Definition patterns
    for i, pattern in enumerate(DEFINITION_PATTERNS):
        matcher.add(f"DEFINITION_{i}", [pattern])
    
    # Obligation patterns
    for i, pattern in enumerate(OBLIGATION_PATTERNS):
        matcher.add(f"OBLIGATION_{i}", [pattern])
    
    # Condition patterns
    for i, pattern in enumerate(CONDITION_PATTERNS):
        matcher.add(f"CONDITION_{i}", [pattern])
    
    # Term patterns
    for i, pattern in enumerate(TERM_PATTERNS):
        matcher.add(f"TERM_{i}", [pattern])
    
    # Payment patterns
    for i, pattern in enumerate(PAYMENT_PATTERNS):
        matcher.add(f"PAYMENT_{i}", [pattern])

# Initialize matchers
initialize_matchers()

def main():
    """Main entry point for the script."""
    parser = argparse.ArgumentParser(
        description="Convert LlamaParse JSON output to Neo4j Cypher commands for graph database import.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Required arguments
    parser.add_argument("input", help="Path to the input JSON file generated by LlamaParse")
    
    args = parser.parse_args()
    
    # Output file name is always derived from the input filename
    base_name = os.path.splitext(args.input)[0]
    output_file = f"{base_name}.cypher"
    
    # Always use en_core_web_sm model
    try:
        global nlp
        nlp = spacy.load("en_core_web_sm")
    except IOError:
        print(f"SpaCy model en_core_web_sm not found. Downloading model...")
        import subprocess
        subprocess.check_call([sys.executable, "-m", "spacy", "download", "en_core_web_sm"])
        nlp = spacy.load("en_core_web_sm")
    
    # Initialize ContractBERT (now required)
    print("Initializing ContractBERT for legal text analysis...")
    if not initialize_contractbert():
        print("Warning: ContractBERT initialization failed. Results may be less accurate.")
        print("Proceeding with SpaCy-only analysis.")
    
    try:
        # Create wrapped extraction functions that include the NLP models
        def extract_metadata_with_models(data):
            return extract_contract_metadata(data, nlp, contractbert_ner, contractbert_classifier)
            
        def extract_articles_with_nlp(data):
            return extract_articles(data, nlp)
            
        def extract_parties_with_nlp(data):
            return extract_parties(data, nlp)
        
        # Call the process_json_file function with our extraction functions
        process_json_file(
            args.input, 
            output_file,
            extract_metadata_with_models,
            extract_articles_with_nlp,
            extract_parties_with_nlp
        )
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
    
if __name__ == "__main__":
    main()