#!/usr/bin/env python3
"""
Contract JSON Enhancer

This script enhances contract JSON data with SpaCy and ContractBERT NLP analysis.
It takes a JSON file (previously generated by LlamaParse) and adds entity recognition
and semantic annotations to improve contract analysis.
"""

import os
import sys
import json
import argparse
from datetime import datetime
import spacy
from transformers import pipeline

# Load NLP models
nlp = spacy.load("en_core_web_lg")
contractbert_ner = None
contractbert_classifier = None

def initialize_nlp_models():
    """Initialize NLP models for contract analysis."""
    global contractbert_ner, contractbert_classifier
    
    print("Loading ContractBERT NER model...")
    contractbert_ner = pipeline("token-classification", 
                               model="nlpaueb/legal-bert-base-uncased", 
                               aggregation_strategy="simple")
    
    print("Loading ContractBERT classifier model...")
    contractbert_classifier = pipeline("text-classification", 
                                     model="nlpaueb/legal-bert-base-uncased")
    
    print("NLP models loaded successfully")

def enhance_contract_json(input_file, output_file=None):
    """Enhance a contract JSON file with advanced NLP models.
    
    Takes a JSON file (previously generated by LlamaParse or similar tool)
    and enhances it with SpaCy and ContractBERT NLP analysis.
    """
    # Initialize NLP models if not already initialized
    if contractbert_ner is None:
        initialize_nlp_models()
    
    # Determine output file if not provided
    if output_file is None:
        base_name = os.path.splitext(input_file)[0]
        output_file = f"{base_name}_enhanced.json"
    
    try:
        # Load the JSON data
        print(f"Loading JSON data from: {input_file}")
        with open(input_file, 'r') as f:
            parsed_data = json.load(f)
        
        # Check if the parsed data is a list (array) or dictionary
        if isinstance(parsed_data, list):
            # If it's a list, enhance each item in the list
            enhanced_data = []
            for item in parsed_data:
                enhanced_item = enhance_with_nlp(item)
                enhanced_data.append(enhanced_item)
        else:
            # If it's a dictionary, enhance it directly
            enhanced_data = enhance_with_nlp(parsed_data)
        
        # Write output to file
        with open(output_file, 'w') as f:
            json.dump(enhanced_data, f, indent=2)
            
        print(f"Successfully enhanced contract data and saved to: {output_file}")
        return enhanced_data
        
    except Exception as e:
        print(f"Error enhancing contract data: {e}")
        sys.exit(1)

def enhance_with_nlp(parsed_data):
    """Enhance parsed contract data with SpaCy and ContractBERT analysis."""
    enhanced_data = parsed_data.copy()
    
    # Add NLP metadata
    enhanced_data["metadata"] = enhanced_data.get("metadata", {})
    enhanced_data["metadata"]["nlp_enhancement"] = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "models": {
            "spacy": "en_core_web_lg",
            "contractbert": "legal-bert-base-uncased"
        }
    }
    
    # Process pages with NLP
    if "pages" in enhanced_data:
        for i, page in enumerate(enhanced_data["pages"]):
            if "text" in page:
                # Process text with SpaCy
                page_text = page["text"]
                doc = nlp(page_text[:10000])  # Limit to 10,000 chars per page for memory
                
                # Extract named entities
                entities = []
                for ent in doc.ents:
                    entities.append({
                        "text": ent.text,
                        "label": ent.label_,
                        "start": ent.start_char,
                        "end": ent.end_char
                    })
                
                # Process with ContractBERT for legal entities
                if len(page_text) > 450:
                    chunks = [page_text[j:j+450] for j in range(0, min(len(page_text), 5000), 450)]
                    for chunk in chunks:
                        bert_entities = contractbert_ner(chunk)
                        for entity in bert_entities:
                            entity_type = entity.get("entity_group", "")
                            if entity_type in ["PERSON", "ORG", "DATE", "MONEY", "LAW"]:
                                entities.append({
                                    "text": entity.get("word", ""),
                                    "label": entity_type,
                                    "start": entity.get("start", 0),
                                    "end": entity.get("end", 0),
                                    "source": "contractbert"
                                })
                else:
                    bert_entities = contractbert_ner(page_text)
                    for entity in bert_entities:
                        entity_type = entity.get("entity_group", "")
                        if entity_type in ["PERSON", "ORG", "DATE", "MONEY", "LAW"]:
                            entities.append({
                                "text": entity.get("word", ""),
                                "label": entity_type,
                                "start": entity.get("start", 0),
                                "end": entity.get("end", 0),
                                "source": "contractbert"
                            })
                
                # Add annotations to the page
                page["nlp_annotations"] = {
                    "entities": entities,
                }
    
    return enhanced_data

def enhance_text_file(txt_file, output_file=None):
    """Enhance text content from a TXT file with NLP analysis."""
    # If output file not specified, create one based on input file name
    if not output_file:
        output_file = os.path.splitext(txt_file)[0] + "_enhanced.json"
    
    try:
        print(f"Reading TXT file: {txt_file}")
        with open(txt_file, 'r', encoding='utf-8') as f:
            text_content = f.read()
        
        # Create a JSON-like structure that the enhance_with_nlp function can work with
        simulated_json = {
            "metadata": {
                "source": "txt_fallback",
                "file": os.path.basename(txt_file),
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            },
            "pages": [{"text": text_content}]
        }
        
        # Enhance with NLP
        enhanced_data = enhance_with_nlp(simulated_json)
        
        # Write output to file
        with open(output_file, 'w') as f:
            json.dump(enhanced_data, f, indent=2)
            
        print(f"Successfully enhanced TXT data and saved to: {output_file}")
        return enhanced_data
        
    except Exception as e:
        print(f"Error enhancing TXT data: {e}")
        sys.exit(1)

def main():
    """Main function to process command line arguments."""
    # Set up argument parser
    parser = argparse.ArgumentParser(description='Enhance contract JSON with NLP analysis')
    parser.add_argument('--json', '-j', type=str, default="./data/sample_contract.json",
                        help='Path to the input JSON file')
    parser.add_argument('--txt', '-t', type=str, default="./data/sample_contract.txt",
                        help='Path to the fallback TXT file if JSON processing fails')
    parser.add_argument('--output', '-o', type=str, default="./data/sample_contract_enhanced.json",
                        help='Path to the output enhanced JSON file')
    
    # Parse arguments
    args = parser.parse_args()
    json_file = args.json
    txt_file = args.txt
    output_file = args.output
    
    # Try to process JSON first
    try:
        print(f"Processing JSON file: {json_file}")
        enhance_contract_json(json_file, output_file)
    except Exception as e:
        # If JSON processing fails, use TXT fallback
        print(f"JSON processing failed: {e}")
        print(f"Using TXT fallback: {txt_file}")
        enhance_text_file(txt_file, output_file)

if __name__ == "__main__":
    main()